2.1 BERT Model：
如图所示，该模型由两部分组成：一个BERT编码器和一个线性层解码器。
在喂给模型前，我们首先对句子进行word to piece操作（例如，将it's分解为it和's），然后根据词典中的序号，将每个piece序号化。
编码器会首先做embedding，将每个单词转换为一个768维的实向量。为了根据句子中各个单词的含义和关系来分析得到句中每个单词的标注，BERT中的transformer结构会综合每个单词的上下文信息，来生成反应单词深层信息的实向量。解码器会根据编码器的输出，得到一个维度为9的实向量，各个维度表示对应预测实体概率的对数值。

2.1.1 Encoder：
我们采用BERT模型来作为编码器，编码对话信息。
记编码器的输出为E=[e_1, e_2, ..., e_N]，其中N为piece句子的长度，e_i∈R……768，每个e_i在原单词的基础上，综合了句子的前后文信息。
注意到在喂给模型前，我们对每个单词进行了word to piece操作，但预测的结果应该是对每个单词的，所以我们在将编码器得到的结果喂给解码器之前，应当先把组成该单词的piece的信息组合起来。具体来说，我们可以取各个piece的第一个、平均值或求和。在本文中取所有piece对应向量的第一个。
设每个word对应piece的角标范围为[(0, p_1), (p_1 + 1, p_2), ..., (p_(M-1) + 1, N)]，其中M为原句的长度。
则我们取[e_0, e_(p_1 + 1), ..., e_(p_(M-1) + 1)]作为编码器的输出，记作F=[f_1, f_2, ..., f_M]

2.1.2 Decoder：
考虑到BERT模型优秀的性能，我们决定直接采用线性层来得到最终的输出。
设D∈R^(768×9)为线性层的权重矩阵，则解码器的输出为A=D×F^T。
由于词性标注任务可以看作是一个多分类认为，因此我们采用负对数似然损失NLLLoss来计算模型预测得到的结果和真实结果之间的误差。
为此，模型得到的应该是各个预测结果概率的对数，即Output=log(Softmax(A))∈R^9。